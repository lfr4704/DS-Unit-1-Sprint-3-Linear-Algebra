{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_134_Clustering_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-3rVFtGLMJM"
      },
      "source": [
        "# K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_VS3FFSFLR3a"
      },
      "source": [
        "# 1) Use the \"Breast Cancer Wisconsin (Diagnostic) Data Set\" from Kaggle to try and cluster types of cancer cells. \n",
        "\n",
        "Here's the original dataset for your reference:\n",
        "\n",
        "<https://www.kaggle.com/uciml/breast-cancer-wisconsin-data>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "899RK3bBn4OE"
      },
      "source": [
        "## This is a supervised learning dataset\n",
        "\n",
        "(Because it has **labels** - The \"diagnosis\" column.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ws5R9X6hLJQ2",
        "outputId": "4e98c7bb-b2f3-46a7-ddce-72d6aba25b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA # You don't necessarily have to use this\n",
        "from sklearn.cluster import KMeans # You don't necessarily have to use this\n",
        "from sklearn.preprocessing import StandardScaler # You don't necessarily have to use this\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/Cancer_Cells.csv\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 33)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IHDDqaU-ove4"
      },
      "source": [
        "## Now it's an unsupervised learning dataset\n",
        "\n",
        "(Because we've removed the diagnosis label) - Use this version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "86MHoPJon_aC",
        "outputId": "4b4bda35-5098-48ad-b706-b8cb6e90eff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "df = df.drop('diagnosis', axis=1)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302        17.99  ...                  0.11890          NaN\n",
              "1    842517        20.57  ...                  0.08902          NaN\n",
              "2  84300903        19.69  ...                  0.08758          NaN\n",
              "3  84348301        11.42  ...                  0.17300          NaN\n",
              "4  84358402        20.29  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toDesdIf0XxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51be81e9-14f7-4afd-d0aa-f163e67bdf24"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09lL4cCek4YY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "aed40517-27f6-4d5f-b30c-68f0d9e66c1c"
      },
      "source": [
        "#check for null/nan values or the KMeans will not run\n",
        "df.isnull().sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                           0\n",
              "radius_mean                  0\n",
              "texture_mean                 0\n",
              "perimeter_mean               0\n",
              "area_mean                    0\n",
              "smoothness_mean              0\n",
              "compactness_mean             0\n",
              "concavity_mean               0\n",
              "concave points_mean          0\n",
              "symmetry_mean                0\n",
              "fractal_dimension_mean       0\n",
              "radius_se                    0\n",
              "texture_se                   0\n",
              "perimeter_se                 0\n",
              "area_se                      0\n",
              "smoothness_se                0\n",
              "compactness_se               0\n",
              "concavity_se                 0\n",
              "concave points_se            0\n",
              "symmetry_se                  0\n",
              "fractal_dimension_se         0\n",
              "radius_worst                 0\n",
              "texture_worst                0\n",
              "perimeter_worst              0\n",
              "area_worst                   0\n",
              "smoothness_worst             0\n",
              "compactness_worst            0\n",
              "concavity_worst              0\n",
              "concave points_worst         0\n",
              "symmetry_worst               0\n",
              "fractal_dimension_worst      0\n",
              "Unnamed: 32                569\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ528WlRlIt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "befff995-8414-4798-b1f9-fd4458a5ac47"
      },
      "source": [
        "#remove nan\n",
        "df = df.drop('Unnamed: 32', axis=1)\n",
        "df.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rskC80k3OKMA"
      },
      "source": [
        "## Let's do it!\n",
        "\n",
        "- You might want to do some data exploration to see if you can find specific columns that will help you find distinct clusters of cells\n",
        "- You might want to use the elbow method to decide on the number of clusters to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTc9WpEjfwGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpldV1sI3bhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data exploration \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0N09TX-3cVC",
        "colab_type": "text"
      },
      "source": [
        "#elbow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CilMb3FN_ztI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "c38a2751-56af-4722-b056-200afc692482"
      },
      "source": [
        "#applying the elbow method\n",
        "#https://stackoverflow.com/questions/19197715/scikit-learn-k-means-elbow-criterion\n",
        "\n",
        "#sum of squared erros/distances SSE\n",
        "#sse = []\n",
        "sse = {}\n",
        "for k in range(1,10):\n",
        "  #print(df)\n",
        "  kmeans = KMeans(n_clusters=k).fit(df)\n",
        "  df['clusters'] = kmeans.labels_ #labels of each point\n",
        "  #print(df['clusters'])\n",
        "  #alternatively\n",
        "  #sse.append(km.inertia_)\n",
        "  sse[k] = kmeans.inertia_ #Inertia: Sum of square distances of samples to their closest cluster center\n",
        "\n",
        "#plot the elbow results\n",
        "plt.figure()\n",
        "#alternatively\n",
        "#plt.plot(k, sse)\n",
        "plt.plot(list(sse.keys()), list(sse.values()), 'bx-')\n",
        "plt.xlabel('Number of cluster')\n",
        "plt.ylabel('SSE')\n",
        "plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAERCAYAAABxZrw0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-AQzIkBlq7r",
        "colab_type": "text"
      },
      "source": [
        "#the elbow is 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7atYYGf8rFrA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "7f081d25-cfd3-43a9-c7dc-adae76f77e19"
      },
      "source": [
        "#Using the silhouette coefficient\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "for n_cluster in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=n_cluster).fit(df)\n",
        "    label = kmeans.labels_\n",
        "    sil_coeff = silhouette_score(df, label, metric='euclidean')\n",
        "    print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For n_clusters=2, The Silhouette Coefficient is 0.9753276628229735\n",
            "For n_clusters=3, The Silhouette Coefficient is 0.9668191442631776\n",
            "For n_clusters=4, The Silhouette Coefficient is 0.9765666110737622\n",
            "For n_clusters=5, The Silhouette Coefficient is 0.9734627271677392\n",
            "For n_clusters=6, The Silhouette Coefficient is 0.9287815815802015\n",
            "For n_clusters=7, The Silhouette Coefficient is 0.9303125364960602\n",
            "For n_clusters=8, The Silhouette Coefficient is 0.932265899352696\n",
            "For n_clusters=9, The Silhouette Coefficient is 0.9381805469431611\n",
            "For n_clusters=10, The Silhouette Coefficient is 0.9305745817912339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgy0zr84sJTf",
        "colab_type": "text"
      },
      "source": [
        "###A higher Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. in this case n_cluster=4 has the hieghest, but here is the catch, our results are anticipating 2 types of cancer results, Malignant and its opposite benign, so therefore we will consider n_cluster=2 as the optimal number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ghqYSxrP_FE"
      },
      "source": [
        "## Check you work: \n",
        "\n",
        "This is something that in a truly unsupervised learning situation **WOULD NOT BE POSSIBLE**. But for educational purposes go back and grab the true diagnosis column (label) from the original dataset. Take your cluster labels and compare them to the original diagnosis column. You can make scatterplots for each to see how they compare or you can calculate a percent accuracy score like: \n",
        "\\begin{align}\n",
        "\\frac{\\text{Num Correct Labels}}{\\text{Num Total Observations}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OIG7-yGLP-eA",
        "colab": {}
      },
      "source": [
        "# Your Code Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BedOTS0eJ9_K"
      },
      "source": [
        "# 2) Perform PCA on your dataset first and *then* use k-means clustering. \n",
        "\n",
        "- You need to standardize your data before PCA.\n",
        "- First try clustering just on PC1 and PC2 so that you can make a scatterplot of your clustering.\n",
        "- Then use use a scree plot to decide how many principal components to include in your clustering, and use however many principal components you need in order to retain 90% of the variation of the original dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dW1AeAK8PNah",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PkrfbzfBROpP"
      },
      "source": [
        "## Check your work: \n",
        "\n",
        "- Compare your PC1, PC2 clustering scatterplot to the clustering scatterplots you made on the raw data\n",
        "- Calculate accuracy scores for both the PC1,PC2 Principal component clustering and the 90% of explained variance clustering.\n",
        "\n",
        "How do your accuracy scores -when preprocessing the data with PCA- compare to the accuracy when simply clustering on the raw data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wKBwVaGOOYsq"
      },
      "source": [
        "# Stretch Goals:\n",
        "\n",
        "- Study for the Sprint Challenge\n",
        "- Work on your Data Storytelling Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9p2djjY5LNWd",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}